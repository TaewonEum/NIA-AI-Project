{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56770599-32d6-4b30-baa4-f33f2b7e4b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40a5125-8d59-41d4-a220-ae118b279e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# origin_path는 원천데이터(이미지데이터)가 저장되어있는 경로로 지정해야합니다.\n",
    "# label_path는 라벨링데이터가 저장되어있는 경로로 지정해야합니다.\n",
    "origin_path='./4.Sample/01.원천데이터'\n",
    "label_path='./4.Sample/02.라벨링데이터'\n",
    "\n",
    "#시드 고정\n",
    "random_seed = 42\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad146ae-3af3-4db4-829b-b3323c8c4aaa",
   "metadata": {},
   "source": [
    "# 데이터 경로 포함 프레임 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20cc8ad-69e5-4dff-a105-fa045ccebcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list=[] #이미지 파일명 까지 경로\n",
    "img_id=[] #이미지 파일명\n",
    "label_list=[] #라벨링 파일명 까지 경로\n",
    "\n",
    "#이미지 파일명 경로 및 이미지 파일명 Get\n",
    "for i in os.listdir(origin_path):\n",
    "    for j in os.listdir(origin_path+'/'+i):\n",
    "        for k in os.listdir(origin_path+'/'+i+'/'+j):\n",
    "            img_id.extend(os.listdir(origin_path+'/'+i+'/'+j+'/'+k))\n",
    "            img_list.extend(glob(origin_path+'/'+i+'/'+j+'/'+k+'/*'))   \n",
    "img_list=[path.replace('\\\\','/') for path in img_list]\n",
    "\n",
    "#라벨링 파일명 및 경로 Get\n",
    "for i in os.listdir(label_path):\n",
    "    for j in os.listdir(label_path+'/'+i):\n",
    "        for k in os.listdir(label_path+'/'+i+'/'+j):\n",
    "            label_list.extend(glob(label_path+'/'+i+'/'+j+'/'+k+'/*'))\n",
    "label_list=[path.replace('\\\\','/') for path in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6685baae-52f0-4ff1-8dc3-c055cd98096d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#데이터 프레임 구축\n",
    "data={'이미지명':img_id,'이미지경로':img_list,'라벨링경로':label_list}\n",
    "data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84770061-e6cb-4f5d-b42a-734b0707e37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class List\n",
    "class_list = ['EG_NA', 'LA_NA', 'PU_NA', 'AB_LI', 'AB_CA', 'AB_BI', 'AB_AP','QB_LI', 'QB_CA', 'QB_BI', 'QB_AP','AA_NA']\n",
    "\n",
    "#라벨링값 채우기\n",
    "label=[]\n",
    "for i in data.index:\n",
    "    img_name=data['이미지명'][i]\n",
    "    matching=[word for word in class_list if word in img_name]\n",
    "    label.append(class_list.index(matching[0]))\n",
    "    \n",
    "data['라벨값']=label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300f17c-98cd-4e8f-b768-a9ce16682e9c",
   "metadata": {},
   "source": [
    "# 데이터 분할 및 분석 데이터 셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f60525-bfcc-4f66-a905-b630450f6af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_test_split 의 stratify 이용해서 분할\n",
    "train, val = train_test_split(data, test_size = 0.1, random_state = 42, stratify = data['라벨값'])\n",
    "train, test= train_test_split(train,test_size = 0.1, random_state = 42, stratify = train['라벨값'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aed1168-192e-48dc-b51b-28d448d30e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans_resize = transforms.Resize((224, 224))\n",
    "trans_tensor = transforms.ToTensor()\n",
    "def dataset(data):\n",
    "    data1=[]\n",
    "    for i in tqdm(data.index):\n",
    "        with open(data['라벨링경로'][i], 'r', encoding='UTF-8') as f:\n",
    "            jsonfile=json.load(f)\n",
    "        if jsonfile['INFO']['DATASET_DETAIL']=='Bounding Box':\n",
    "            for j in jsonfile['ANNOTATION_INFO']:\n",
    "                x_data = trans_tensor(trans_resize(Image.open(data['이미지경로'][i]).crop((j['XTL'], j['YTL'], j['XBR'], j['YBR']))))\n",
    "                y_data= data['라벨값'][i]\n",
    "                data_list=[[x_data,y_data,jsonfile['IMAGE']['IMAGE_FILE_NAME']+'_'+str((jsonfile['ANNOTATION_INFO'].index(j))+1)]]\n",
    "                data1.extend(data_list)\n",
    "            \n",
    "        elif jsonfile['INFO']['DATASET_DETAIL']=='Polygon':\n",
    "            for m in jsonfile['ANNOTATION_INFO']:\n",
    "                XTL = min(m['POLYGON'][0::2])\n",
    "                XBR = max(m['POLYGON'][0::2])\n",
    "                YTL = min(m['POLYGON'][0::2])\n",
    "                YBR = max(m['POLYGON'][0::2])\n",
    "                x_data = trans_tensor(trans_resize(Image.open(data['이미지경로'][i]).crop((XTL, YTL, XBR, YBR))))\n",
    "                y_data = data['라벨값'][i]\n",
    "                data_list=[[x_data,y_data,jsonfile['IMAGE']['IMAGE_FILE_NAME']+'_'+str((jsonfile['ANNOTATION_INFO'].index(m))+1)]]\n",
    "                data1.extend(data_list)\n",
    "    print(f'{len(data1)}개 업로드 완료')\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "914ac744-1ad3-44ef-a8e7-d2f082e364cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1377/1377 [00:34<00:00, 39.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4280개 업로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471개 업로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 153/153 [00:03<00:00, 38.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466개 업로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traindataset = dataset(data=train)\n",
    "valdataset = dataset(data=val)\n",
    "testdataset = dataset(data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad927dfc-4146-4b4e-a080-d2188d720ef2",
   "metadata": {},
   "source": [
    "# Train Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fa1774-0c4a-41c0-a0f1-a7970aace836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수, default = 3\n",
    "    use_auxiliary (boolean) : Auxiliary의 사용 여부, default = True\n",
    "    num_classes (integer)   : Input dataset 의 class(label)의 개수, default = 10\n",
    "    \n",
    "    GoogleNet 모형의 전체 구조 구현\n",
    "    '''\n",
    "    def __init__(self,num_classes, in_channels=3, use_auxiliary=True):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.use_auxiliary = use_auxiliary\n",
    "        if use_auxiliary:\n",
    "            self.auxiliary4a = Auxiliary(512, num_classes)\n",
    "            self.auxiliary4d = Auxiliary(528, num_classes)\n",
    "        \n",
    "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = None\n",
    "        z = None\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            y = self.auxiliary4a(x)\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            z = self.auxiliary4d(x)\n",
    "        \n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d734e40-bd9e-4bab-96b1-e8da8e5c496f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수\n",
    "    out_channels (integer)  : Output dataset 의 channel 수\n",
    "    kernel_size (integer) : Convolution Layer의 kernel 크기\n",
    "    \n",
    "    GoogleNet 모형의 Inception Module에 사용되는 Convolution Block을 구현\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654c8679-caed-4e5b-b713-d5710603ca62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수\n",
    "    num_1x1(integer)        : Filter의 크기가 1X1인 Convolution Block(self.one_by_one)의 출력 channel 수\n",
    "    num_3x3_red(integer)    : Filter의 크기가 1X1인 Convolution Block(self.tree_by_three_red)의 출력 channel 수\n",
    "    num_3x3(integer)        : Filter의 크기가 3X3인 Convolution Block(self.tree_by_three)의 출력 channel 수\n",
    "    num_5x5_red(integer)    : Filter의 크기가 1X1인 Convolution Block(self.five_by_five_red)의 출력 channel 수\n",
    "    num_5x5(integer)        : Filter의 크기가 5X5인 Convolution Block(self.five_by_five)의 출력 channel 수\n",
    "    num_pool_proj(integer)  : Filter의 크기가 1X1인 Convolution Block(self.pool_proj)의 출력 channel 수\n",
    "    \n",
    "    GoogleNet 모형의 Inception Module을 구현\n",
    "    '''\n",
    "    def __init__(self, in_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.one_by_one = ConvBlock(in_channels, num_1x1, kernel_size=1)\n",
    "        \n",
    "        self.tree_by_three_red = ConvBlock(in_channels, num_3x3_red, kernel_size=1)  \n",
    "        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.five_by_five_red = ConvBlock(in_channels, num_5x5_red, kernel_size=1)\n",
    "        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_proj = ConvBlock(in_channels, num_pool_proj, kernel_size=1)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x1 = self.one_by_one(x)\n",
    "        \n",
    "        x2 = self.tree_by_three_red(x)\n",
    "        x2 = self.tree_by_three(x2)\n",
    "        \n",
    "        x3 = self.five_by_five_red(x)\n",
    "        x3 = self.five_by_five(x3)\n",
    "        \n",
    "        x4 = self.maxpool(x)\n",
    "        x4 = self.pool_proj(x4)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3, x4], 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90502b08-8d4d-42b9-a880-1927572f0cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Auxiliary(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수          \n",
    "    num_classes (integer)   : Input dataset 의 class(label)의 개수 \n",
    "    \n",
    "    GoogleNet 모형에서 train dataset 학습시 사용되는 Auxiliary Classifier를 구현\n",
    "    '''\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Auxiliary, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "325c8e0b-05f3-49b4-a8f9-8f2d37261942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GoogleNetModel(nn.Module):\n",
    "    '''\n",
    "    train_dataset (dataset)         : 모형 학습에서 train dataset 로 사용할 데이터 셋\n",
    "    val_dataset (dataset)           : 모형 학습에서 validation dataset 로 사용할 데이터 셋\n",
    "    epoch (integer)              : 학습을 반복할 횟수\n",
    "    learning_rate (float)        : 모형 학습에서 사용할 learning_rate\n",
    "    batch_size (integer)         : 한 번 학습에 사용할 이미지의 개수\n",
    "    loader_num_workers (integer) : DataLoader에서 사용할 코어의 개수\n",
    "        \n",
    "    GoogleNet 모형을 학습시키는 일련의 과정을 구현\n",
    "    '''\n",
    "    def __init__(self, train_dataset, val_dataset, num_classes, epoch, learning_rate, batch_size, loader_num_workers):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset   \n",
    "        self.num_classes = num_classes\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.loader_num_workers = loader_num_workers\n",
    "        self.patience_i = int(min(np.ceil(epoch*0.1), 10))\n",
    "        \n",
    "        \n",
    "    def GoogleNetTrain(self):\n",
    "        model = Inception(num_classes = self.num_classes).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.patience_i, verbose=True)\n",
    "        \n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.loader_num_workers)\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.loader_num_workers)\n",
    "        dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "                \n",
    "        since = time.time()\n",
    "        train_acc_history = []\n",
    "        train_loss_history = []\n",
    "        val_acc_history = []\n",
    "        val_loss_history = []\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        use_auxiliary=True\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            epoch_time = time.time()\n",
    "            print('Epoch {}/{}'.format(i+1, self.epoch))\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']: # Each epoch has a training and validation phase\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]: # Iterate over dataset\n",
    "\n",
    "                    inputs = inputs.to(self.device)\n",
    "\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
    "\n",
    "                        if phase == 'train': # Backward + optimize only if in training phase\n",
    "                            if use_auxiliary:\n",
    "                                outputs, aux1, aux2 = model(inputs)\n",
    "                                loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
    "                            else:\n",
    "                                outputs, _, _ = model(inputs)\n",
    "                                loss = criterion(outputs, labels)\n",
    "\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        if phase == 'val':\n",
    "                            outputs, _, _ = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "                if phase == 'val': # Adjust learning rate based on val loss\n",
    "                    lr_scheduler.step(epoch_loss)\n",
    "\n",
    "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "                \n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "                e_time = time.time() - epoch_time\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_num = i+1\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(float(epoch_acc))\n",
    "                    val_loss_history.append(float(epoch_loss))\n",
    "                if phase == 'train':\n",
    "                    train_acc_history.append(float(epoch_acc))\n",
    "                    train_loss_history.append(float(epoch_loss))\n",
    "\n",
    "            print('Epoch Time : {:.0f}m {:.0f}s'.format(e_time//60, e_time%60))\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best Model is {0}'.format(best_num))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        return model, self.epoch, train_loss_history, train_acc_history, val_loss_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69139854-3934-4278-940f-5500d7358953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "train Loss: 2.8716 Acc: 0.3963\n",
      "val Loss: 6.0402 Acc: 0.2272\n",
      "Epoch Time : 0m 23s\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n",
      "train Loss: 2.1227 Acc: 0.4984\n",
      "val Loss: 2.5518 Acc: 0.4437\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n",
      "train Loss: 1.7572 Acc: 0.6033\n",
      "val Loss: 3.0752 Acc: 0.2675\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n",
      "train Loss: 1.6725 Acc: 0.6269\n",
      "val Loss: 1.0707 Acc: 0.6030\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n",
      "train Loss: 1.4739 Acc: 0.6685\n",
      "val Loss: 0.9293 Acc: 0.6518\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n",
      "train Loss: 1.4158 Acc: 0.6752\n",
      "val Loss: 1.3793 Acc: 0.6072\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n",
      "train Loss: 1.4425 Acc: 0.6738\n",
      "val Loss: 1.1391 Acc: 0.5860\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n",
      "train Loss: 1.3416 Acc: 0.6916\n",
      "val Loss: 0.9025 Acc: 0.6858\n",
      "Epoch Time : 0m 21s\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n",
      "train Loss: 1.2887 Acc: 0.6928\n",
      "val Loss: 1.2002 Acc: 0.5924\n",
      "Epoch Time : 0m 21s\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n",
      "train Loss: 1.1903 Acc: 0.7231\n",
      "val Loss: 1.3898 Acc: 0.5754\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n",
      "train Loss: 1.2145 Acc: 0.7194\n",
      "val Loss: 1.0394 Acc: 0.6624\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n",
      "train Loss: 1.1703 Acc: 0.7315\n",
      "val Loss: 1.5286 Acc: 0.5563\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n",
      "train Loss: 1.0967 Acc: 0.7498\n",
      "val Loss: 0.8448 Acc: 0.6900\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n",
      "train Loss: 1.0607 Acc: 0.7554\n",
      "val Loss: 0.8874 Acc: 0.7176\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n",
      "train Loss: 0.9754 Acc: 0.7692\n",
      "val Loss: 2.1021 Acc: 0.5478\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n",
      "train Loss: 1.0362 Acc: 0.7617\n",
      "val Loss: 1.0973 Acc: 0.6645\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 17/50\n",
      "----------\n",
      "train Loss: 1.1515 Acc: 0.7467\n",
      "val Loss: 0.8473 Acc: 0.7028\n",
      "Epoch Time : 0m 21s\n",
      "\n",
      "Epoch 18/50\n",
      "----------\n",
      "train Loss: 1.0263 Acc: 0.7659\n",
      "val Loss: 0.7935 Acc: 0.7601\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 19/50\n",
      "----------\n",
      "train Loss: 0.9802 Acc: 0.7731\n",
      "val Loss: 1.1048 Acc: 0.6837\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 20/50\n",
      "----------\n",
      "train Loss: 0.9463 Acc: 0.7836\n",
      "val Loss: 1.0362 Acc: 0.6348\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 21/50\n",
      "----------\n",
      "train Loss: 0.9406 Acc: 0.7843\n",
      "val Loss: 1.1807 Acc: 0.6603\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 22/50\n",
      "----------\n",
      "train Loss: 0.8861 Acc: 0.7998\n",
      "val Loss: 1.2677 Acc: 0.6476\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 23/50\n",
      "----------\n",
      "train Loss: 0.8691 Acc: 0.8012\n",
      "val Loss: 0.7334 Acc: 0.7792\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 24/50\n",
      "----------\n",
      "train Loss: 0.8357 Acc: 0.8072\n",
      "val Loss: 0.6373 Acc: 0.7941\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 25/50\n",
      "----------\n",
      "train Loss: 0.7708 Acc: 0.8194\n",
      "val Loss: 0.6257 Acc: 0.8132\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 26/50\n",
      "----------\n",
      "train Loss: 0.8097 Acc: 0.8124\n",
      "val Loss: 1.0611 Acc: 0.6964\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 27/50\n",
      "----------\n",
      "train Loss: 0.7668 Acc: 0.8206\n",
      "val Loss: 0.6676 Acc: 0.7771\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 28/50\n",
      "----------\n",
      "train Loss: 0.7692 Acc: 0.8248\n",
      "val Loss: 1.0813 Acc: 0.7452\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 29/50\n",
      "----------\n",
      "train Loss: 0.7525 Acc: 0.8311\n",
      "val Loss: 0.7027 Acc: 0.7643\n",
      "Epoch Time : 0m 21s\n",
      "\n",
      "Epoch 30/50\n",
      "----------\n",
      "train Loss: 0.7887 Acc: 0.8264\n",
      "val Loss: 1.2008 Acc: 0.6921\n",
      "Epoch Time : 0m 22s\n",
      "\n",
      "Epoch 31/50\n",
      "----------\n",
      "train Loss: 0.7039 Acc: 0.8437\n",
      "Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n",
      "val Loss: 2.2584 Acc: 0.5159\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 32/50\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.8619\n",
      "val Loss: 0.4370 Acc: 0.8344\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 33/50\n",
      "----------\n",
      "train Loss: 0.5544 Acc: 0.8773\n",
      "val Loss: 0.4123 Acc: 0.8832\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 34/50\n",
      "----------\n",
      "train Loss: 0.5346 Acc: 0.8818\n",
      "val Loss: 0.4301 Acc: 0.8747\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 35/50\n",
      "----------\n",
      "train Loss: 0.5247 Acc: 0.8834\n",
      "val Loss: 0.4102 Acc: 0.8832\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 36/50\n",
      "----------\n",
      "train Loss: 0.5235 Acc: 0.8813\n",
      "val Loss: 0.3988 Acc: 0.8747\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 37/50\n",
      "----------\n",
      "train Loss: 0.5065 Acc: 0.8876\n",
      "val Loss: 0.4073 Acc: 0.8769\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 38/50\n",
      "----------\n",
      "train Loss: 0.4978 Acc: 0.8902\n",
      "val Loss: 0.4062 Acc: 0.8811\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 39/50\n",
      "----------\n",
      "train Loss: 0.4927 Acc: 0.8916\n",
      "val Loss: 0.4038 Acc: 0.8832\n",
      "Epoch Time : 0m 21s\n",
      "\n",
      "Epoch 40/50\n",
      "----------\n",
      "train Loss: 0.4771 Acc: 0.8928\n",
      "val Loss: 0.4107 Acc: 0.8832\n",
      "Epoch Time : 0m 22s\n",
      "\n",
      "Epoch 41/50\n",
      "----------\n",
      "train Loss: 0.4876 Acc: 0.8909\n",
      "val Loss: 0.3913 Acc: 0.8811\n",
      "Epoch Time : 0m 21s\n",
      "\n",
      "Epoch 42/50\n",
      "----------\n",
      "train Loss: 0.4779 Acc: 0.8951\n",
      "val Loss: 0.4063 Acc: 0.8747\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 43/50\n",
      "----------\n",
      "train Loss: 0.4674 Acc: 0.8958\n",
      "val Loss: 0.4191 Acc: 0.8790\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 44/50\n",
      "----------\n",
      "train Loss: 0.4649 Acc: 0.8991\n",
      "val Loss: 0.3973 Acc: 0.8875\n",
      "Epoch Time : 0m 24s\n",
      "\n",
      "Epoch 45/50\n",
      "----------\n",
      "train Loss: 0.4620 Acc: 0.9002\n",
      "val Loss: 0.3962 Acc: 0.8875\n",
      "Epoch Time : 0m 22s\n",
      "\n",
      "Epoch 46/50\n",
      "----------\n",
      "train Loss: 0.4613 Acc: 0.8998\n",
      "val Loss: 0.3880 Acc: 0.8832\n",
      "Epoch Time : 0m 22s\n",
      "\n",
      "Epoch 47/50\n",
      "----------\n",
      "train Loss: 0.4530 Acc: 0.9026\n",
      "val Loss: 0.3990 Acc: 0.8854\n",
      "Epoch Time : 0m 22s\n",
      "\n",
      "Epoch 48/50\n",
      "----------\n",
      "train Loss: 0.4512 Acc: 0.8995\n",
      "val Loss: 0.4081 Acc: 0.8811\n",
      "Epoch Time : 0m 22s\n",
      "\n",
      "Epoch 49/50\n",
      "----------\n",
      "train Loss: 0.4413 Acc: 0.9019\n",
      "val Loss: 0.3873 Acc: 0.8875\n",
      "Epoch Time : 0m 23s\n",
      "\n",
      "Epoch 50/50\n",
      "----------\n",
      "train Loss: 0.4397 Acc: 0.9000\n",
      "val Loss: 0.3889 Acc: 0.8875\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Training complete in 17m 12s\n",
      "Best Model is 44\n",
      "Best val Acc: 0.887473\n"
     ]
    }
   ],
   "source": [
    "model, epoch, train_loss_history, train_acc_history, val_loss_history, val_acc_history = GoogleNetModel(train_dataset = [i[0:2] for i in traindataset],\n",
    "                                                                                                        val_dataset = [i[0:2] for i in valdataset],\n",
    "                                                                                                        num_classes = 12, epoch = 50, \n",
    "                                                                                                        learning_rate = 0.005, batch_size = 64,\n",
    "                                                                                                        loader_num_workers = 2).GoogleNetTrain()\n",
    "if (os.path.exists('./models')==False):\n",
    "    os.makedirs('./models')\n",
    "torch.save(model, f'./models/GoogleNet_Lifecycle_AA_Final.pt')\n",
    "torch.save(model.state_dict(), f'./models/GoogleNet_stat_Lifecycle_AA_Final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8f6e8b-8b33-4890-9595-c07d814ccf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNet Test 결과 출력 ( 2023-12-05 16:41:39.421482 )\n",
      "cuda로 결과를 출력중입니다. cpu로 계산할 경우 시간이 오래 걸리니 기다려주세요. ( 2023-12-05 16:41:39.422480 )\n",
      "\n",
      "Accuracy for class : EG_NA   is 91.49 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : LA_NA   is 93.42 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : PU_NA   is 76.70 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : AB_LI   is 55.56 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : AB_CA   is 86.21 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : AB_BI   is 68.75 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : AB_AP   is 46.43 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : QB_LI   is 92.86 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : QB_CA   is 92.86 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : QB_BI   is 46.15 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : QB_AP   is 84.62 % ( 2023-12-05 16:41:41.206477 )\n",
      "Accuracy for class : AA_NA   is 100.00 % ( 2023-12-05 16:41:41.206477 )\n",
      "\n",
      "Total classes : 77.92 % ( 2023-12-05 16:41:41.206477 )\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "except:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "try:\n",
    "    torch.backends.cudnn.deterministic = True    \n",
    "    torch.backends.cudnn.benchmark = False \n",
    "except:\n",
    "    pass\n",
    "np.random.seed(random_seed)\n",
    "try:\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = torch.load('./models/GoogleNet_Lifecycle_AA_Final.pt', map_location=device)\n",
    "model.load_state_dict(torch.load('./models/GoogleNet_stat_Lifecycle_AA_Final.pt', map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "test_loader = DataLoader([x[:3] for x in testdataset], batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "label_list = list()\n",
    "prediction_list = list()\n",
    "\n",
    "print('GoogleNet Test 결과 출력', '(', datetime.datetime.now(), ')')\n",
    "\n",
    "classes = ['EG_NA', 'LA_NA', 'PU_NA', 'AB_LI', 'AB_CA', 'AB_BI', 'AB_AP','QB_LI', 'QB_CA', 'QB_BI', 'QB_AP','AA_NA']\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "print(f'{device}로 결과를 출력중입니다. cpu로 계산할 경우 시간이 오래 걸리니 기다려주세요.', '(', datetime.datetime.now(), ')\\n')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels, _ = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predictions = torch.max(outputs[0], 1)    \n",
    "\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            label_list.append(label.tolist())\n",
    "            prediction_list.append(prediction.tolist())\n",
    "mean = 0\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class : {classname:7s} is {accuracy:.2f} %', '(', datetime.datetime.now(), ')')\n",
    "    mean += accuracy\n",
    "print(f'\\nTotal classes : {mean/len(classes):.2f} %', '(', datetime.datetime.now(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531b06e-f5e1-4b91-8866-0a8432bd0b65",
   "metadata": {},
   "source": [
    "# 유효성 증빙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9568a36-2f20-4b67-919c-79b467a54ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 개별 결과값 Test data set\n",
    "\n",
    "test_ID=[]\n",
    "for i in range(len(testdataset)):\n",
    "    ID=testdataset[i][2]\n",
    "    test_ID.append(ID)\n",
    "\n",
    "    \n",
    "test_ID=pd.DataFrame(test_ID)\n",
    "label_data=pd.DataFrame(label_list)\n",
    "pred_data=pd.DataFrame(prediction_list)\n",
    "\n",
    "df_list=pd.concat([test_ID,label_data],axis=1)\n",
    "df_list=pd.concat([df_list,pred_data],axis=1)\n",
    "df_list.columns=['file_name','label','prediction']\n",
    "df_list\n",
    "\n",
    "# 문제당 개별 결과값 저장\n",
    "df_list.to_csv('./filelist'+'문제 당 개별 결과값.csv',encoding='cp949',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8b197-185c-4721-b01f-a3d4c7f44059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. confusion matrix\n",
    "\n",
    "cf=confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "cf=pd.DataFrame(cf).rename(index={0:'True_백묵병음성',1:'True_백묵병양성'}\n",
    "                             ,columns={0:'Pred_백묵병음성',1:'pred_백묵병양성'})\n",
    "#계산할 때 사용된 값 저장 (Confusion matrix 기반 TP, FP, TN, FN)\n",
    "cf.to_csv('./filelist'+'Confusion Matrix 기반 TP, FP, TN, FN(백묵병 분류 모델).csv',encoding='cp949')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
