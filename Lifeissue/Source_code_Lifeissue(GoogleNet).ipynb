{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49a3aae-d2dc-46e2-a15a-9ac2793f3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c108e247-4652-4820-9a82-ecbb8c460210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_path는 원천데이터(이미지데이터)가 저장되어있는 경로로 지정해야합니다.\n",
    "# label_path는 라벨링데이터가 저장되어있는 경로로 지정해야합니다.\n",
    "origin_path='./4.Sample/01.원천데이터'\n",
    "label_path='./4.Sample/02.라벨링데이터'\n",
    "\n",
    "#시드 고정\n",
    "random_seed = 42\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d86a28-c21c-4bf2-9f53-b9379fed0d38",
   "metadata": {},
   "source": [
    "# 데이터 경로 포함 프레임 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2930a13-e94c-4df9-ba8c-1680ef2f55ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list=[] #이미지 파일명 까지 경로\n",
    "img_id=[] #이미지 파일명\n",
    "label_list=[] #라벨링 파일명 까지 경로\n",
    "\n",
    "#이미지 파일명 경로 및 이미지 파일명 Get\n",
    "for i in os.listdir(origin_path):\n",
    "    for j in os.listdir(origin_path+'/'+i):\n",
    "        for k in os.listdir(origin_path+'/'+i+'/'+j):\n",
    "            if (k=='02.애벌레') or (k=='12.생애이슈(백묵병)'): #02.애벌레=음성, 12.생애이슈(백묵병)=양성\n",
    "                img_id.extend(os.listdir(origin_path+'/'+i+'/'+j+'/'+k))\n",
    "                img_list.extend(glob(origin_path+'/'+i+'/'+j+'/'+k+'/*'))   \n",
    "img_list=[path.replace('\\\\','/') for path in img_list]\n",
    "\n",
    "#라벨링 파일명 및 경로 Get\n",
    "for i in os.listdir(label_path):\n",
    "    for j in os.listdir(label_path+'/'+i):\n",
    "        for k in os.listdir(label_path+'/'+i+'/'+j):\n",
    "            if (k=='02.애벌레') or (k=='12.생애이슈(백묵병)'):\n",
    "                label_list.extend(glob(label_path+'/'+i+'/'+j+'/'+k+'/*'))\n",
    "label_list=[path.replace('\\\\','/') for path in label_list]\n",
    "\n",
    "#데이터 프레임 구축\n",
    "data={'이미지명':img_id,'이미지경로':img_list,'라벨링경로':label_list}\n",
    "data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32129ac-368c-4975-a018-e302fa5d934c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class List\n",
    "class_list = ['LA_NA', 'AA_NA'] #음성, 양성\n",
    "\n",
    "#라벨링값 채우기\n",
    "label=[]\n",
    "for i in data.index:\n",
    "    img_name=data['이미지명'][i]\n",
    "    matching=[word for word in class_list if word in img_name]\n",
    "    label.append(class_list.index(matching[0]))\n",
    "    \n",
    "data['라벨값']=label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a3971-f1fa-47e6-9fdd-703a0bf52012",
   "metadata": {},
   "source": [
    "# 데이터 분할 및 분석 데이터 셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f948723-e584-42fe-b68b-4943b735f86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_test_split 의 stratify 이용해서 분할\n",
    "train, val = train_test_split(data, test_size = 0.1, random_state = 42, stratify = data['라벨값'])\n",
    "train, test= train_test_split(train,test_size = 0.1, random_state = 42, stratify = train['라벨값'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946e5661-566a-4491-9ef8-dccdfd4a1113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>이미지명</th>\n",
       "      <th>이미지경로</th>\n",
       "      <th>라벨링경로</th>\n",
       "      <th>라벨값</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>06_1_R_AA_NA_20221102_04_0626.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/1...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>01_1_R_LA_NA_20220913_01_0318.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/02.Polygon/02.애벌레...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/02.Polygon/02.애벌...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>01_1_R_LA_NA_20220720_07_0532.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/0...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>01_1_D_LA_NA_20220817_12_0057.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/02.파괴데이터/01.Bounding Box/0...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/02.파괴데이터/01.Bounding Box/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>01_1_R_LA_NA_20220907_01_0433.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/02.Polygon/02.애벌레...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/02.Polygon/02.애벌...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>01_1_D_LA_NA_20220812_10_0355.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/02.파괴데이터/01.Bounding Box/0...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/02.파괴데이터/01.Bounding Box/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>06_1_R_AA_NA_20221102_03_0412.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/1...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>01_1_D_LA_NA_20220818_04_0246.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/02.파괴데이터/01.Bounding Box/0...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/02.파괴데이터/01.Bounding Box/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>06_1_R_AA_NA_20221102_03_0333.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/1...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>01_1_R_LA_NA_20220907_01_0708.jpg</td>\n",
       "      <td>./4.Sample/01.원천데이터/01.실제데이터/02.Polygon/02.애벌레...</td>\n",
       "      <td>./4.Sample/02.라벨링데이터/01.실제데이터/02.Polygon/02.애벌...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  이미지명  \\\n",
       "61   06_1_R_AA_NA_20221102_04_0626.jpg   \n",
       "147  01_1_R_LA_NA_20220913_01_0318.jpg   \n",
       "29   01_1_R_LA_NA_20220720_07_0532.jpg   \n",
       "185  01_1_D_LA_NA_20220817_12_0057.jpg   \n",
       "125  01_1_R_LA_NA_20220907_01_0433.jpg   \n",
       "..                                 ...   \n",
       "160  01_1_D_LA_NA_20220812_10_0355.jpg   \n",
       "58   06_1_R_AA_NA_20221102_03_0412.jpg   \n",
       "198  01_1_D_LA_NA_20220818_04_0246.jpg   \n",
       "57   06_1_R_AA_NA_20221102_03_0333.jpg   \n",
       "133  01_1_R_LA_NA_20220907_01_0708.jpg   \n",
       "\n",
       "                                                 이미지경로  \\\n",
       "61   ./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/1...   \n",
       "147  ./4.Sample/01.원천데이터/01.실제데이터/02.Polygon/02.애벌레...   \n",
       "29   ./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/0...   \n",
       "185  ./4.Sample/01.원천데이터/02.파괴데이터/01.Bounding Box/0...   \n",
       "125  ./4.Sample/01.원천데이터/01.실제데이터/02.Polygon/02.애벌레...   \n",
       "..                                                 ...   \n",
       "160  ./4.Sample/01.원천데이터/02.파괴데이터/01.Bounding Box/0...   \n",
       "58   ./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/1...   \n",
       "198  ./4.Sample/01.원천데이터/02.파괴데이터/01.Bounding Box/0...   \n",
       "57   ./4.Sample/01.원천데이터/01.실제데이터/01.Bounding Box/1...   \n",
       "133  ./4.Sample/01.원천데이터/01.실제데이터/02.Polygon/02.애벌레...   \n",
       "\n",
       "                                                 라벨링경로  라벨값  \n",
       "61   ./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...    1  \n",
       "147  ./4.Sample/02.라벨링데이터/01.실제데이터/02.Polygon/02.애벌...    0  \n",
       "29   ./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...    0  \n",
       "185  ./4.Sample/02.라벨링데이터/02.파괴데이터/01.Bounding Box/...    0  \n",
       "125  ./4.Sample/02.라벨링데이터/01.실제데이터/02.Polygon/02.애벌...    0  \n",
       "..                                                 ...  ...  \n",
       "160  ./4.Sample/02.라벨링데이터/02.파괴데이터/01.Bounding Box/...    0  \n",
       "58   ./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...    1  \n",
       "198  ./4.Sample/02.라벨링데이터/02.파괴데이터/01.Bounding Box/...    0  \n",
       "57   ./4.Sample/02.라벨링데이터/01.실제데이터/01.Bounding Box/...    1  \n",
       "133  ./4.Sample/02.라벨링데이터/01.실제데이터/02.Polygon/02.애벌...    0  \n",
       "\n",
       "[162 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a09c10-b1bc-45d0-a9ce-74d1bdc291cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans_resize = transforms.Resize((224, 224))\n",
    "trans_tensor = transforms.ToTensor()\n",
    "def dataset(data):\n",
    "    data1=[]\n",
    "    for i in tqdm(data.index):\n",
    "        with open(data['라벨링경로'][i], 'r', encoding='UTF-8') as f:\n",
    "            jsonfile=json.load(f)\n",
    "        if jsonfile['INFO']['DATASET_DETAIL']=='Bounding Box':\n",
    "            for j in jsonfile['ANNOTATION_INFO']:\n",
    "                x_data = trans_tensor(trans_resize(Image.open(data['이미지경로'][i]).crop((j['XTL'], j['YTL'], j['XBR'], j['YBR']))))\n",
    "                y_data= data['라벨값'][i]\n",
    "                data_list=[[x_data,y_data,jsonfile['IMAGE']['IMAGE_FILE_NAME']+'_'+str((jsonfile['ANNOTATION_INFO'].index(j))+1)]]\n",
    "                data1.extend(data_list)\n",
    "            \n",
    "        elif jsonfile['INFO']['DATASET_DETAIL']=='Polygon':\n",
    "            for m in jsonfile['ANNOTATION_INFO']:\n",
    "                XTL = min(m['POLYGON'][0::2])\n",
    "                XBR = max(m['POLYGON'][0::2])\n",
    "                YTL = min(m['POLYGON'][0::2])\n",
    "                YBR = max(m['POLYGON'][0::2])\n",
    "                x_data = trans_tensor(trans_resize(Image.open(data['이미지경로'][i]).crop((XTL, YTL, XBR, YBR))))\n",
    "                y_data = data['라벨값'][i]\n",
    "                data_list=[[x_data,y_data,jsonfile['IMAGE']['IMAGE_FILE_NAME']+'_'+str((jsonfile['ANNOTATION_INFO'].index(m))+1)]]\n",
    "                data1.extend(data_list)\n",
    "    print(f'{len(data1)}개 업로드 완료')\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a86682-52fa-49b1-b25f-1333db4bdec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 162/162 [00:10<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282개 업로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137개 업로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 17.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125개 업로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traindataset = dataset(data=train)\n",
    "valdataset = dataset(data=val)\n",
    "testdataset = dataset(data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08444ad0-49c3-4a0d-998b-309399e8a70a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.2314, 0.2314, 0.2314,  ..., 0.7412, 0.7490, 0.7490],\n",
       "          [0.2314, 0.2314, 0.2314,  ..., 0.7412, 0.7490, 0.7490],\n",
       "          [0.2392, 0.2392, 0.2392,  ..., 0.7255, 0.7333, 0.7333],\n",
       "          ...,\n",
       "          [0.3059, 0.3059, 0.3059,  ..., 0.8745, 0.8745, 0.8745],\n",
       "          [0.3020, 0.3020, 0.3020,  ..., 0.8745, 0.8745, 0.8745],\n",
       "          [0.3020, 0.3020, 0.3020,  ..., 0.8745, 0.8745, 0.8745]],\n",
       " \n",
       "         [[0.1020, 0.1020, 0.1020,  ..., 0.4392, 0.4431, 0.4431],\n",
       "          [0.1020, 0.1020, 0.1020,  ..., 0.4392, 0.4431, 0.4431],\n",
       "          [0.1059, 0.1059, 0.1059,  ..., 0.4235, 0.4314, 0.4314],\n",
       "          ...,\n",
       "          [0.1373, 0.1373, 0.1373,  ..., 0.5608, 0.5608, 0.5608],\n",
       "          [0.1373, 0.1373, 0.1373,  ..., 0.5608, 0.5608, 0.5608],\n",
       "          [0.1373, 0.1373, 0.1373,  ..., 0.5608, 0.5608, 0.5608]],\n",
       " \n",
       "         [[0.0431, 0.0431, 0.0431,  ..., 0.2549, 0.2549, 0.2549],\n",
       "          [0.0431, 0.0431, 0.0431,  ..., 0.2549, 0.2549, 0.2549],\n",
       "          [0.0471, 0.0471, 0.0471,  ..., 0.2431, 0.2471, 0.2471],\n",
       "          ...,\n",
       "          [0.0745, 0.0745, 0.0745,  ..., 0.3608, 0.3608, 0.3608],\n",
       "          [0.0745, 0.0745, 0.0745,  ..., 0.3608, 0.3608, 0.3608],\n",
       "          [0.0745, 0.0745, 0.0745,  ..., 0.3608, 0.3608, 0.3608]]]),\n",
       " 0,\n",
       " '01_1_R_LA_NA_20220715_04_2491.jpg_9']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset[400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39834632-4aaa-426f-9dca-6e33361630ab",
   "metadata": {},
   "source": [
    "# Train Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7423bc37-cc4c-4d31-97b9-e26ffe1e4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수, default = 3\n",
    "    use_auxiliary (boolean) : Auxiliary의 사용 여부, default = True\n",
    "    num_classes (integer)   : Input dataset 의 class(label)의 개수, default = 10\n",
    "    \n",
    "    GoogleNet 모형의 전체 구조 구현\n",
    "    '''\n",
    "    def __init__(self,num_classes, in_channels=3, use_auxiliary=True):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.use_auxiliary = use_auxiliary\n",
    "        if use_auxiliary:\n",
    "            self.auxiliary4a = Auxiliary(512, num_classes)\n",
    "            self.auxiliary4d = Auxiliary(528, num_classes)\n",
    "        \n",
    "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = None\n",
    "        z = None\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            y = self.auxiliary4a(x)\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            z = self.auxiliary4d(x)\n",
    "        \n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d91635-56e5-474c-a13d-092941112143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수\n",
    "    out_channels (integer)  : Output dataset 의 channel 수\n",
    "    kernel_size (integer) : Convolution Layer의 kernel 크기\n",
    "    \n",
    "    GoogleNet 모형의 Inception Module에 사용되는 Convolution Block을 구현\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76d01c17-6841-46a1-8058-7a286d41d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수\n",
    "    num_1x1(integer)        : Filter의 크기가 1X1인 Convolution Block(self.one_by_one)의 출력 channel 수\n",
    "    num_3x3_red(integer)    : Filter의 크기가 1X1인 Convolution Block(self.tree_by_three_red)의 출력 channel 수\n",
    "    num_3x3(integer)        : Filter의 크기가 3X3인 Convolution Block(self.tree_by_three)의 출력 channel 수\n",
    "    num_5x5_red(integer)    : Filter의 크기가 1X1인 Convolution Block(self.five_by_five_red)의 출력 channel 수\n",
    "    num_5x5(integer)        : Filter의 크기가 5X5인 Convolution Block(self.five_by_five)의 출력 channel 수\n",
    "    num_pool_proj(integer)  : Filter의 크기가 1X1인 Convolution Block(self.pool_proj)의 출력 channel 수\n",
    "    \n",
    "    GoogleNet 모형의 Inception Module을 구현\n",
    "    '''\n",
    "    def __init__(self, in_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.one_by_one = ConvBlock(in_channels, num_1x1, kernel_size=1)\n",
    "        \n",
    "        self.tree_by_three_red = ConvBlock(in_channels, num_3x3_red, kernel_size=1)  \n",
    "        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.five_by_five_red = ConvBlock(in_channels, num_5x5_red, kernel_size=1)\n",
    "        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_proj = ConvBlock(in_channels, num_pool_proj, kernel_size=1)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x1 = self.one_by_one(x)\n",
    "        \n",
    "        x2 = self.tree_by_three_red(x)\n",
    "        x2 = self.tree_by_three(x2)\n",
    "        \n",
    "        x3 = self.five_by_five_red(x)\n",
    "        x3 = self.five_by_five(x3)\n",
    "        \n",
    "        x4 = self.maxpool(x)\n",
    "        x4 = self.pool_proj(x4)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3, x4], 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac64e9d-6762-4c54-bf95-d79b75107a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary(nn.Module):\n",
    "    '''\n",
    "    in_channels (integer)   : Input dataset 의 channel 수          \n",
    "    num_classes (integer)   : Input dataset 의 class(label)의 개수 \n",
    "    \n",
    "    GoogleNet 모형에서 train dataset 학습시 사용되는 Auxiliary Classifier를 구현\n",
    "    '''\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Auxiliary, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "479ba2cc-1efe-414a-bcde-173949b2e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNetModel(nn.Module):\n",
    "    '''\n",
    "    train_dataset (dataset)         : 모형 학습에서 train dataset 로 사용할 데이터 셋\n",
    "    val_dataset (dataset)           : 모형 학습에서 validation dataset 로 사용할 데이터 셋\n",
    "    epoch (integer)              : 학습을 반복할 횟수\n",
    "    learning_rate (float)        : 모형 학습에서 사용할 learning_rate\n",
    "    batch_size (integer)         : 한 번 학습에 사용할 이미지의 개수\n",
    "    loader_num_workers (integer) : DataLoader에서 사용할 코어의 개수\n",
    "        \n",
    "    GoogleNet 모형을 학습시키는 일련의 과정을 구현\n",
    "    '''\n",
    "    def __init__(self, train_dataset, val_dataset, num_classes, epoch, learning_rate, batch_size, loader_num_workers):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset   \n",
    "        self.num_classes = num_classes\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.loader_num_workers = loader_num_workers\n",
    "        self.patience_i = int(min(np.ceil(epoch*0.1), 10))\n",
    "        \n",
    "        \n",
    "    def GoogleNetTrain(self):\n",
    "        model = Inception(num_classes = self.num_classes).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.patience_i, verbose=True)\n",
    "        \n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.loader_num_workers)\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.loader_num_workers)\n",
    "        dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "                \n",
    "        since = time.time()\n",
    "        train_acc_history = []\n",
    "        train_loss_history = []\n",
    "        val_acc_history = []\n",
    "        val_loss_history = []\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        use_auxiliary=True\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            epoch_time = time.time()\n",
    "            print('Epoch {}/{}'.format(i+1, self.epoch))\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']: # Each epoch has a training and validation phase\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]: # Iterate over dataset\n",
    "\n",
    "                    inputs = inputs.to(self.device)\n",
    "\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
    "\n",
    "                        if phase == 'train': # Backward + optimize only if in training phase\n",
    "                            if use_auxiliary:\n",
    "                                outputs, aux1, aux2 = model(inputs)\n",
    "                                loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
    "                            else:\n",
    "                                outputs, _, _ = model(inputs)\n",
    "                                loss = criterion(outputs, labels)\n",
    "\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        if phase == 'val':\n",
    "                            outputs, _, _ = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "                if phase == 'val': # Adjust learning rate based on val loss\n",
    "                    lr_scheduler.step(epoch_loss)\n",
    "\n",
    "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "                \n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "                e_time = time.time() - epoch_time\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_num = i+1\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(float(epoch_acc))\n",
    "                    val_loss_history.append(float(epoch_loss))\n",
    "                if phase == 'train':\n",
    "                    train_acc_history.append(float(epoch_acc))\n",
    "                    train_loss_history.append(float(epoch_loss))\n",
    "\n",
    "            print('Epoch Time : {:.0f}m {:.0f}s'.format(e_time//60, e_time%60))\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best Model is {0}'.format(best_num))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        return model, self.epoch, train_loss_history, train_acc_history, val_loss_history, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc500c5-f633-4c6b-bf94-63320d1a2feb",
   "metadata": {},
   "source": [
    "# Model Training 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50c2f521-35ee-409a-badc-5fe24d05c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 6.8020 Acc: 0.8019\n",
      "val Loss: 0.3222 Acc: 0.8540\n",
      "Epoch Time : 0m 26s\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 0.8006 Acc: 0.8495\n",
      "val Loss: 0.0875 Acc: 0.9562\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.7440 Acc: 0.8658\n",
      "val Loss: 0.1435 Acc: 0.9708\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.8041 Acc: 0.8666\n",
      "val Loss: 0.3228 Acc: 0.9781\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.7903 Acc: 0.8456\n",
      "Epoch 00005: reducing learning rate of group 0 to 5.0000e-03.\n",
      "val Loss: 1.1948 Acc: 0.5766\n",
      "Epoch Time : 0m 23s\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.6936 Acc: 0.8799\n",
      "val Loss: 0.1709 Acc: 0.9635\n",
      "Epoch Time : 0m 23s\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.6940 Acc: 0.8721\n",
      "val Loss: 0.1866 Acc: 0.9416\n",
      "Epoch Time : 0m 23s\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.6308 Acc: 0.9072\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
      "val Loss: 0.2003 Acc: 0.9343\n",
      "Epoch Time : 0m 24s\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.9204\n",
      "val Loss: 0.1493 Acc: 0.9635\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.9321\n",
      "val Loss: 0.1447 Acc: 0.9635\n",
      "Epoch Time : 0m 20s\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.9477\n",
      "Epoch 00011: reducing learning rate of group 0 to 5.0000e-05.\n",
      "val Loss: 0.1237 Acc: 0.9854\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.5430 Acc: 0.9602\n",
      "val Loss: 0.1697 Acc: 0.9562\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.5591 Acc: 0.9602\n",
      "val Loss: 0.1406 Acc: 0.9708\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.5335 Acc: 0.9696\n",
      "Epoch 00014: reducing learning rate of group 0 to 5.0000e-06.\n",
      "val Loss: 0.1740 Acc: 0.9562\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.5283 Acc: 0.9688\n",
      "val Loss: 0.1480 Acc: 0.9781\n",
      "Epoch Time : 0m 19s\n",
      "\n",
      "Training complete in 5m 15s\n",
      "Best Model is 11\n",
      "Best val Acc: 0.985401\n"
     ]
    }
   ],
   "source": [
    "model, epoch, train_loss_history, train_acc_history, val_loss_history, val_acc_history = GoogleNetModel(train_dataset = [i[0:2] for i in traindataset],\n",
    "                                                                                                        val_dataset = [i[0:2] for i in valdataset],\n",
    "                                                                                                        num_classes = 2, epoch = 15, \n",
    "                                                                                                        learning_rate = 0.05, batch_size = 4,\n",
    "                                                                                                        loader_num_workers = 2).GoogleNetTrain()\n",
    "if (os.path.exists('./models')==False):\n",
    "    os.makedirs('./models')\n",
    "torch.save(model, f'./models/GoogleNet_Lifeissue_AA_Final.pt')\n",
    "torch.save(model.state_dict(), f'./models/GoogleNet_stat_Lifeissue_AA_Final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9e1c1-0a65-4752-9535-0aa51e94b3e2",
   "metadata": {},
   "source": [
    "# Model Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40559471-6167-47e5-91e9-17f4b7e15fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNet Test 결과 출력 ( 2023-12-27 11:34:40.678927 )\n",
      "cuda로 결과를 출력중입니다. cpu로 계산할 경우 시간이 오래 걸리니 기다려주세요.\n",
      "Accuracy for class : LA_NEG  is 100.00 % ( 2023-12-27 11:34:43.304009 )\n",
      "Accuracy for class : LA_POS  is 100.00 % ( 2023-12-27 11:34:43.305011 )\n",
      "\n",
      "Total classes : 100.00 % ( 2023-12-27 11:34:43.305011 )\n"
     ]
    }
   ],
   "source": [
    "random_seed=42\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "except:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "try:\n",
    "    torch.backends.cudnn.deterministic = True    \n",
    "    torch.backends.cudnn.benchmark = False \n",
    "except:\n",
    "    pass\n",
    "np.random.seed(random_seed)\n",
    "try:\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#### Test Accuracy for classes\n",
    "model = torch.load('./models/GoogleNet_Lifeissue_AA_Final.pt', map_location=device)\n",
    "model.load_state_dict(torch.load('./models/GoogleNet_stat_Lifeissue_AA_Final.pt', map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "test_loader = DataLoader(testdataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "label_list = list()\n",
    "prediction_list = list()\n",
    "\n",
    "#### Print Result\n",
    "print('GoogleNet Test 결과 출력', '(', datetime.datetime.now(), ')')\n",
    "\n",
    "\n",
    "classes = ['LA_NEG','LA_POS']\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "print(f'{device}로 결과를 출력중입니다. cpu로 계산할 경우 시간이 오래 걸리니 기다려주세요.')\n",
    "\n",
    "model.eval()\n",
    "#logger.info('GoogleNet 모델의 Test를 진행합니다.')\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels, _ = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predictions = torch.max(outputs[0], 1)    \n",
    "\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            label_list.append(label.tolist())\n",
    "            prediction_list.append(prediction.tolist())\n",
    "mean = 0\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "\n",
    "    print(f'Accuracy for class : {classname:7s} is {accuracy:.2f} %', '(', datetime.datetime.now(), ')')\n",
    "    mean += accuracy\n",
    "print(f'\\nTotal classes : {mean/len(classes):.2f} %', '(', datetime.datetime.now(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ce84c-d7df-4c7d-a614-008d45f986d2",
   "metadata": {},
   "source": [
    "# 유효성 증빙서류 \n",
    "\n",
    "1. 개별 결과값\n",
    "\n",
    "2. confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88b0c2a1-9df3-4796-85a6-bb7b50df03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 개별 결과값 Test data set\n",
    "\n",
    "test_ID=[]\n",
    "for i in range(len(testdataset)):\n",
    "    ID=testdataset[i][2]\n",
    "    test_ID.append(ID)\n",
    "\n",
    "    \n",
    "test_ID=pd.DataFrame(test_ID)\n",
    "label_data=pd.DataFrame(label_list)\n",
    "pred_data=pd.DataFrame(prediction_list)\n",
    "\n",
    "df_list=pd.concat([test_ID,label_data],axis=1)\n",
    "df_list=pd.concat([df_list,pred_data],axis=1)\n",
    "df_list.columns=['file_name','label','prediction']\n",
    "df_list\n",
    "\n",
    "# 문제당 개별 결과값 저장\n",
    "df_list.to_csv('./filelist'+'문제 당 개별 결과값.csv',encoding='cp949',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0882bd8e-f23b-4936-8cc7-2e3eecfe8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. confusion matrix\n",
    "\n",
    "cf=confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "cf=pd.DataFrame(cf).rename(index={0:'True_백묵병음성',1:'True_백묵병양성'}\n",
    "                             ,columns={0:'Pred_백묵병음성',1:'pred_백묵병양성'})\n",
    "#계산할 때 사용된 값 저장 (Confusion matrix 기반 TP, FP, TN, FN)\n",
    "cf.to_csv('./filelist'+'Confusion Matrix 기반 TP, FP, TN, FN(백묵병 분류 모델).csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ff834-78b7-4562-aae0-c7b70115f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
